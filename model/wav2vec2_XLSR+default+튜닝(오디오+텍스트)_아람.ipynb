{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byahram/119/blob/master/model/wav2vec2_XLSR%2Bdefault%2B%ED%8A%9C%EB%8B%9D(%EC%98%A4%EB%94%94%EC%98%A4%2B%ED%85%8D%EC%8A%A4%ED%8A%B8)_%EC%95%84%EB%9E%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I_Eh69jOqJT1"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install transformers torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAWOKnQeOtz"
      },
      "source": [
        "## 마운트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQesRLXWeMbx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/Projects/LikeLion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-4NAL4gj25V"
      },
      "outputs": [],
      "source": [
        "# TRAIN_CSV_PATH = \"./train/train.csv\"\n",
        "# TRAIN_AUDIO_TAR = \"./train/train_audio.tar\"\n",
        "# VALID_CSV_PATH = \"./validation/validation.csv\"\n",
        "# VALID_AUDIO_TAR = \"./validation/valid_audio.tar\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCn_H5qEeR5s"
      },
      "source": [
        "## 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS08FBd1qcV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import ast\n",
        "from typing import List, Dict\n",
        "import io\n",
        "import tarfile\n",
        "import weakref\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, Wav2Vec2Model, AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 1. 환경 설정 및 랜덤 시드 고정\n",
        "# =========================================\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "TEXT_MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
        "TRAIN_CSV_PATH = \"./train/train.csv\"\n",
        "TRAIN_AUDIO_TAR = \"./train/train_audio.tar\"\n",
        "VALID_CSV_PATH = \"./validation/validation.csv\"\n",
        "VALID_AUDIO_TAR = \"./validation/valid_audio.tar\"\n",
        "MAX_TEXT_LEN = 128\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_AUDIO_FRAMES = 400\n",
        "MIN_AUDIO_FRAMES = 8\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device : {device}\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 2. 텍스트 전처리 함수\n",
        "# =========================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
        "\n",
        "def preprocess_text(text: str, tokenizer, max_length: int = MAX_TEXT_LEN) -> Dict[str, torch.Tensor]:\n",
        "    if isinstance(text, str) and len(text.strip()) > 0:\n",
        "        sentences = [s.strip().strip('\"') for s in text.split(',') if s.strip()]\n",
        "    else:\n",
        "        sentences = []\n",
        "\n",
        "    joined_text = ' [SEP] '.join(sentences) if sentences else ''\n",
        "    enc = tokenizer(\n",
        "        joined_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return {k: v.squeeze(0) for k, v in enc.items()}\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 3. wav2vec2-XLSR 모델 초기화\n",
        "# =========================================\n",
        "W2V_MODEL_NAME = \"facebook/wav2vec2-xls-r-300m\"  # 멀티링구얼 모델 (한국어 포함)\n",
        "w2v_fe = AutoFeatureExtractor.from_pretrained(W2V_MODEL_NAME)\n",
        "w2v_model = Wav2Vec2Model.from_pretrained(W2V_MODEL_NAME).to(device)\n",
        "\n",
        "# === 수정된 부분: 마지막 2개 레이어만 파인 튜닝 ===\n",
        "# 1. masking 관련 오류 방지: feature extractor mask disable\n",
        "w2v_fe.do_masking = False\n",
        "\n",
        "# w2v_model을 평가 모드로 설정하고 동결\n",
        "w2v_model.eval()\n",
        "for p in w2v_model.parameters():\n",
        "    p.requires_grad = False\n",
        "# =====================================================\n",
        "\n",
        "# 샘플링 레이트\n",
        "SAMPLE_RATE = 16000\n",
        "MIN_AUDIO_FRAMES = 8\n",
        "MAX_AUDIO_FRAMES = 400\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 4. Dataset 정의\n",
        "# =========================================\n",
        "class TextAudioDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, meta_cols: List[str], audio_tar_path: str,\n",
        "                 urgency_map: Dict[str, int], sentiment_map: Dict[str, int], meta_maps: Dict[str, Dict]):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.meta_cols = meta_cols\n",
        "        self.audio_tar_path = audio_tar_path\n",
        "        self.urgency_map = urgency_map\n",
        "        self.sentiment_map = sentiment_map\n",
        "\n",
        "        # 메타데이터 인덱스 매핑\n",
        "        self.meta_maps = meta_maps\n",
        "        self.meta_vocab_sizes = {col: len(self.meta_maps.get(col, {})) + 1 for col in meta_cols}\n",
        "\n",
        "        # 오디오 로딩 (tar 내부)\n",
        "        self.tar = None\n",
        "        self.member_map = {}\n",
        "        self.audio_cache = {}  # {basename: bytes}\n",
        "\n",
        "        if audio_tar_path and os.path.exists(audio_tar_path):\n",
        "            mode = 'r:gz' if audio_tar_path.endswith(('.tar.gz', '.tgz')) else 'r'\n",
        "            self.tar = tarfile.open(audio_tar_path, mode)\n",
        "            # 필요한 순간에 extractfile 하려고 인덱스만 만든다.\n",
        "            for m in self.tar.getmembers():\n",
        "                if m.isfile():\n",
        "                    self.member_map[os.path.basename(m.name)] = m\n",
        "        # tar는 객체 수명 끝나면 닫히도록\n",
        "        self._finalizer = weakref.finalize(self, self._cleanup)\n",
        "\n",
        "\n",
        "    def _cleanup(self):\n",
        "        try:\n",
        "            if self.tar is not None:\n",
        "                self.tar.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # -----------------------------\n",
        "        # 1) 텍스트 처리\n",
        "        # -----------------------------\n",
        "        text_enc = preprocess_text(row.get('text', ''), tokenizer, MAX_TEXT_LEN)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 2) wav2vec2-XLSR 오디오 처리\n",
        "        # -----------------------------\n",
        "        audio_paths = []\n",
        "        if 'audio_split_list' in row and pd.notna(row['audio_split_list']):\n",
        "            try:\n",
        "                audio_paths = ast.literal_eval(row['audio_split_list'])\n",
        "                if not isinstance(audio_paths, (list, tuple)):\n",
        "                    audio_paths = []\n",
        "            except Exception:\n",
        "                audio_paths = []\n",
        "\n",
        "        features = []\n",
        "        for audio_file in audio_paths:\n",
        "            audio_bytes = self.audio_cache.get(audio_file)\n",
        "            if audio_bytes is None and self.tar is not None:\n",
        "                m = self.member_map.get(audio_file)\n",
        "                if m is not None:\n",
        "                    f = self.tar.extractfile(m)\n",
        "                    if f:\n",
        "                        audio_bytes = f.read()\n",
        "                        self.audio_cache[audio_file] = audio_bytes  # 1번 읽은 건 캐시\n",
        "            if audio_bytes is None:\n",
        "                features.append(None)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                waveform, sr = torchaudio.load(io.BytesIO(audio_bytes))  # (ch, T)\n",
        "            except Exception:\n",
        "                features.append(None)\n",
        "                continue\n",
        "\n",
        "            # mono + 16k\n",
        "            if waveform.dim() == 2 and waveform.size(0) > 1:\n",
        "                waveform = waveform.mean(dim=0, keepdim=True)\n",
        "            if sr != SAMPLE_RATE:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fe_out = w2v_fe(\n",
        "                    waveform.squeeze(0).numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\"\n",
        "                )\n",
        "                input_values = fe_out.input_values.to(device)  # (1, T')\n",
        "                out = w2v_model(input_values)                 # (1, t, d)\n",
        "                hs = out.last_hidden_state.squeeze(0)         # (t, d)\n",
        "                hs = hs.transpose(0, 1).contiguous()          # (d, t)\n",
        "\n",
        "            # 프레임 패딩/자르기 (기존 하이퍼 재사용)\n",
        "            frames = hs.size(1)\n",
        "            if frames < MAX_AUDIO_FRAMES:\n",
        "                hs = torch.nn.functional.pad(hs, (0, MAX_AUDIO_FRAMES - frames))  # ← 무조건 MAX에 맞추기\n",
        "            else:\n",
        "                hs = hs[:, :MAX_AUDIO_FRAMES]\n",
        "            features.append(hs)\n",
        "\n",
        "        # 여러 조각 평균, 없으면 0-텐서\n",
        "        D = w2v_model.config.hidden_size  # 예: 1024(300m)\n",
        "\n",
        "        # None이 아닌 유효한 피처만 필터링해서 그 피처들로만 평균값\n",
        "        valid_features = [f for f in features if f is not None]\n",
        "\n",
        "        if not valid_features:      # 유효한 피처가 하나도 없으면\n",
        "            audio_feat = torch.zeros(D, MAX_AUDIO_FRAMES, device=device)\n",
        "        else:\n",
        "            # 2. 유효한 피처들만 스택하고 평균\n",
        "            # (루프 안에서 이미 MAX_AUDIO_FRAMES로 패딩/잘라내기 되었음)\n",
        "            audio_feat = torch.stack(valid_features, dim=0).mean(dim=0)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 3) 메타데이터 처리\n",
        "        # -----------------------------\n",
        "        meta_idx = []\n",
        "        for col in self.meta_cols:\n",
        "            val = row.get(col, None)\n",
        "            idx_val = 0 if pd.isna(val) or val is None else self.meta_maps.get(col, {}).get(val, 0)\n",
        "            vocab_size = self.meta_vocab_sizes.get(col, 1)\n",
        "            idx_val = max(0, min(idx_val, vocab_size-1))\n",
        "            meta_idx.append(idx_val)\n",
        "        meta_idx = torch.tensor(meta_idx, dtype=torch.long) if meta_idx else torch.tensor([0], dtype=torch.long)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 4) Labels 처리\n",
        "        # -----------------------------\n",
        "        urg_val = row.get('urgencyLevel', None)\n",
        "        urgency = self.urgency_map.get(urg_val, self.urgency_map.get(str(urg_val), 0))\n",
        "\n",
        "        try:\n",
        "            urgency = int(urgency)\n",
        "        except Exception:\n",
        "            urgency = 0\n",
        "        urgency = torch.tensor(urgency, dtype=torch.long)\n",
        "\n",
        "        sent_val = row.get('sentiment', None)\n",
        "        sentiment_idx = self.sentiment_map.get(sent_val, self.sentiment_map.get(str(sent_val), 0))\n",
        "\n",
        "        num_classes = max(1, len(self.sentiment_map))\n",
        "        sentiment_idx = max(0, min(sentiment_idx, num_classes-1))\n",
        "        sentiment = torch.tensor(sentiment_idx, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': text_enc['input_ids'].to(device),\n",
        "            'attention_mask': text_enc['attention_mask'].to(device),\n",
        "            'audio_feat': audio_feat.to(device),\n",
        "            'meta_idx': meta_idx.to(device),\n",
        "            'urgency': urgency.to(device),\n",
        "            'sentiment': sentiment.to(device)\n",
        "        }\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 3-1. collate_fn: dynamic padding\n",
        "# =========================================\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    meta_idx = torch.stack([item['meta_idx'] for item in batch])\n",
        "    urgency = torch.stack([item['urgency'] for item in batch])\n",
        "    sentiment = torch.stack([item['sentiment'] for item in batch])\n",
        "\n",
        "    audio_feats = [item['audio_feat'] for item in batch]\n",
        "    max_frames = max([feat.size(1) for feat in audio_feats])\n",
        "    padded_audio = [torch.nn.functional.pad(feat, (0, max_frames - feat.size(1))) for feat in audio_feats]\n",
        "    audio_feat = torch.stack(padded_audio)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'audio_feat': audio_feat,\n",
        "        'meta_idx': meta_idx,\n",
        "        'urgency': urgency,\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 4. 모델 정의 (wav2vec2-XLSR용)\n",
        "# =========================================\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_model_name, meta_maps: Dict[str, Dict],\n",
        "                 audio_emb_dim=256, joint_dim=256,\n",
        "                 num_classes_urgency=3,\n",
        "                 num_classes_sentiment=4,\n",
        "                 audio_input_dim=None):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
        "\n",
        "        # === 수정된 부분: 마지막 2개 레이어만 파인 튜닝 ===\n",
        "        for name, param in self.text_encoder.named_parameters():\n",
        "            if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "        # =============================================\n",
        "\n",
        "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, joint_dim)\n",
        "\n",
        "        if audio_input_dim is None:\n",
        "            audio_input_dim = w2v_model.config.hidden_size  # wav2vec2-XLSR의 hidden dim\n",
        "\n",
        "        self.audio_proj_shared   = nn.Linear(audio_input_dim, audio_emb_dim)\n",
        "        self.audio_proj_urgency  = nn.Linear(audio_emb_dim, joint_dim)\n",
        "        self.audio_proj_sentiment= nn.Linear(audio_emb_dim, joint_dim)\n",
        "\n",
        "        # --- Meta Embeddings ---\n",
        "        self.meta_embs = nn.ModuleDict()\n",
        "        for k, mapping in meta_maps.items():\n",
        "            num_embeddings = len(mapping) + 1\n",
        "            emb_dim = min(8, num_embeddings)\n",
        "            self.meta_embs[k] = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=emb_dim, padding_idx=0)\n",
        "        self.meta_out_dim = sum([emb.embedding_dim for emb in self.meta_embs.values()]) if self.meta_embs else 0\n",
        "\n",
        "        # concat 방식으로 변경: joint_dim * 2 (텍스트 + 오디오)\n",
        "        self.urgency_head = nn.Linear(joint_dim * 2 + self.meta_out_dim, num_classes_urgency)\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(joint_dim * 2 + self.meta_out_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_sentiment)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_feat, meta_idx):\n",
        "        B = input_ids.size(0)\n",
        "        # --- Text Encoder ---\n",
        "        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_emb = self.text_proj(txt_out.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # --- Audio Encoder ---\n",
        "        # audio_feat: (B, D, T)\n",
        "        audio_mean = audio_feat.mean(dim=2)  # (B, D)\n",
        "        audio_emb = self.audio_proj_shared(audio_mean)\n",
        "        audio_emb_urgency  = self.audio_proj_urgency(audio_emb)\n",
        "        audio_emb_sentiment= self.audio_proj_sentiment(audio_emb)\n",
        "\n",
        "        # --- Meta Embedding ---\n",
        "        if not self.meta_embs:\n",
        "            meta_emb = torch.zeros(B, 0, device=input_ids.device)\n",
        "        else:\n",
        "            if meta_idx.dim() == 1:\n",
        "                meta_idx = meta_idx.unsqueeze(0).expand(B, -1)\n",
        "            meta_embeddings = [self.meta_embs[k](meta_idx[:, i].clamp(0, self.meta_embs[k].num_embeddings-1))\n",
        "                               for i, k in enumerate(self.meta_embs.keys())]\n",
        "            meta_emb = torch.cat(meta_embeddings, dim=1)\n",
        "\n",
        "        # sum 대신 concat으로 변경\n",
        "        joint_urgency   = torch.cat([txt_emb, audio_emb_urgency], dim=1)\n",
        "        joint_sentiment = torch.cat([txt_emb, audio_emb_sentiment], dim=1)\n",
        "\n",
        "        urg_out = self.urgency_head(torch.cat([joint_urgency, meta_emb], dim=1))\n",
        "        sent_out = self.sentiment_head(torch.cat([joint_sentiment, meta_emb], dim=1))\n",
        "\n",
        "        # # --- Fusion: concat text + audio + meta ---\n",
        "        # fusion_urgency = torch.cat([txt_emb, audio_emb_urgency, meta_emb], dim=1)\n",
        "        # fusion_sentiment = torch.cat([txt_emb, audio_emb_sentiment, meta_emb], dim=1)\n",
        "\n",
        "        # # --- Fusion MLP 통과 ---\n",
        "        # joint_urgency = self.fusion_mlp_urgency(fusion_urgency)\n",
        "        # joint_sentiment = self.fusion_mlp_sentiment(fusion_sentiment)\n",
        "\n",
        "        # # --- Output Heads ---\n",
        "        # urg_out = self.urgency_head(joint_urgency)\n",
        "        # sent_out = self.sentiment_head(joint_sentiment)\n",
        "\n",
        "        return urg_out, sent_out\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 5. 학습 함수\n",
        "# =========================================\n",
        "def train_model(df_train, df_valid, meta_cols, train_tar, valid_tar,\n",
        "                urgency_map, sentiment_map, meta_maps,\n",
        "                num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE):\n",
        "\n",
        "    # meta_maps = {col: {v: (i + 1) for i, v in enumerate(df_train[col].dropna().unique())} for col in meta_cols}\n",
        "    train_dataset = TextAudioDataset(df_train, meta_cols, train_tar, urgency_map, sentiment_map, meta_maps)\n",
        "    valid_dataset = TextAudioDataset(df_valid, meta_cols, valid_tar, urgency_map, sentiment_map, meta_maps)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = MultimodalClassifier(\n",
        "        TEXT_MODEL_NAME, meta_maps,\n",
        "        num_classes_urgency=len(urgency_map),\n",
        "        num_classes_sentiment=len(sentiment_map),\n",
        "        audio_input_dim=w2v_model.config.hidden_size\n",
        "    ).to(device)\n",
        "\n",
        "    # === 수정된 부분: 학습할 파라미터 그룹화 ===\n",
        "    # 1. MultimodalClassifier의 파라미터 (여기엔 동결된 KcELECTRA 제외)\n",
        "    params_to_train = list(model.parameters())\n",
        "\n",
        "    # 2. w2v_model에서 requires_grad=True인 파라미터만 추가\n",
        "    params_to_train.extend([p for p in w2v_model.parameters() if p.requires_grad])\n",
        "\n",
        "    optimizer = optim.AdamW(params_to_train, lr=1e-4) # <--- 수정된 파라미터 전달\n",
        "    # ========================================\n",
        "\n",
        "    criterion_urg = nn.CrossEntropyLoss()\n",
        "    criterion_sent = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses, train_urg_losses, train_sent_losses = [], [], []\n",
        "    valid_losses, valid_urg_losses, valid_sent_losses = [], [], []\n",
        "    urg_acc_list, urg_f1_list, sent_acc_list, sent_f1_list = [], [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss, total_train_urg, total_train_sent = 0.0, 0.0, 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            pred_urg, pred_sent = model(batch['input_ids'], batch['attention_mask'],\n",
        "                                        batch['audio_feat'], batch['meta_idx'])\n",
        "            urg_loss = criterion_urg(pred_urg, batch['urgency'])\n",
        "            sent_loss = criterion_sent(pred_sent, batch['sentiment'])\n",
        "            loss = urg_loss + sent_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_urg += urg_loss.item()\n",
        "            total_train_sent += sent_loss.item()\n",
        "\n",
        "        train_losses.append(total_train_loss / len(train_loader))\n",
        "        train_urg_losses.append(total_train_urg / len(train_loader))\n",
        "        train_sent_losses.append(total_train_sent / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        total_valid_loss, total_valid_urg, total_valid_sent = 0, 0, 0\n",
        "        all_urg_pred, all_urg_true = [], []\n",
        "        all_sent_pred, all_sent_true = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1} [Valid]\"):\n",
        "                pred_urg, pred_sent = model(batch['input_ids'], batch['attention_mask'],\n",
        "                                            batch['audio_feat'], batch['meta_idx'])\n",
        "                urg_loss = criterion_urg(pred_urg, batch['urgency'])\n",
        "                sent_loss = criterion_sent(pred_sent, batch['sentiment'])\n",
        "                loss = urg_loss + sent_loss\n",
        "\n",
        "                total_valid_loss += loss.item()\n",
        "                total_valid_urg += urg_loss.item()\n",
        "                total_valid_sent += sent_loss.item()\n",
        "\n",
        "                all_urg_pred.extend(pred_urg.argmax(dim=1).cpu().numpy())\n",
        "                all_urg_true.extend(batch['urgency'].cpu().numpy())\n",
        "                all_sent_pred.extend(pred_sent.argmax(dim=1).cpu().numpy())\n",
        "                all_sent_true.extend(batch['sentiment'].cpu().numpy())\n",
        "\n",
        "        valid_losses.append(total_valid_loss / len(valid_loader))\n",
        "        valid_urg_losses.append(total_valid_urg / len(valid_loader))\n",
        "        valid_sent_losses.append(total_valid_sent / len(valid_loader))\n",
        "\n",
        "        urg_acc = accuracy_score(all_urg_true, all_urg_pred)\n",
        "        urg_f1 = f1_score(all_urg_true, all_urg_pred, average='weighted')\n",
        "        sent_acc = accuracy_score(all_sent_true, all_sent_pred) # (변수명만 명확하게 변경)\n",
        "        sent_f1 = f1_score(all_sent_true, all_sent_pred, average='weighted') # (변수명만 명확하게 변경)\n",
        "\n",
        "        urg_acc_list.append(urg_acc)\n",
        "        urg_f1_list.append(urg_f1)\n",
        "        sent_acc_list.append(sent_acc)\n",
        "        sent_f1_list.append(sent_f1)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_losses[-1]:.4f} | Valid Loss: {valid_losses[-1]:.4f}\")\n",
        "        print(f\"Train Urg Loss: {train_urg_losses[-1]:.4f} | Train Sent Loss: {train_sent_losses[-1]:.4f}\")\n",
        "        print(f\"Valid Urg Loss: {valid_urg_losses[-1]:.4f} | Valid Sent Loss: {valid_sent_losses[-1]:.4f}\")\n",
        "        print(f\"Urgency -> Accuracy: {urg_acc:.4f}, Weighted F1: {urg_f1:.4f}\")\n",
        "        print(f\"Sentiment -> Accuracy: {sent_acc:.4f}, Weighted F1: {sent_f1:.4f}\\n\")\n",
        "\n",
        "    scores = {\n",
        "        'train_loss': train_losses, 'train_urg_loss': train_urg_losses, 'train_sent_loss': train_sent_losses,\n",
        "        'valid_loss': valid_losses, 'valid_urg_loss': valid_urg_losses, 'valid_sent_loss': valid_sent_losses,\n",
        "        'urg_accuracy': urg_acc_list, 'urg_f1': urg_f1_list,\n",
        "        'sent_accuracy': sent_acc_list, 'sent_f1': sent_f1_list\n",
        "    }\n",
        "    return model, scores\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 6. 학습 결과 시각화\n",
        "# =========================================\n",
        "def plot_scores(scores):\n",
        "    epochs = range(1, len(scores['train_loss'])+1)\n",
        "    plt.figure(figsize=(14,6))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, scores['train_urg_loss'], label='Train Urg Loss')\n",
        "    plt.plot(epochs, scores['train_sent_loss'], label='Train Sent Loss')\n",
        "    plt.plot(epochs, scores['valid_urg_loss'], '--', label='Valid Urg Loss')\n",
        "    plt.plot(epochs, scores['valid_sent_loss'], '--', label='Valid Sent Loss')\n",
        "    plt.title('Task Losses')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, scores['urg_accuracy'], label='Urg Accuracy')\n",
        "    plt.plot(epochs, scores['urg_f1'], label='Urg F1')\n",
        "    plt.plot(epochs, scores['sent_accuracy'], label='Sent Accuracy')\n",
        "    plt.plot(epochs, scores['sent_f1'], label='Sent F1')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0_MLlnQqcS7"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 7. 학습 실행\n",
        "# =========================================\n",
        "df = pd.read_csv(TRAIN_CSV_PATH)\n",
        "\n",
        "# 데이터 'disasterLarge' 비율 맞춰 추출\n",
        "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
        "LABEL_COL = \"disasterLarge\"  # 비율 기준 컬럼명\n",
        "TARGET_N = 500               # 다운샘플 목표 개수\n",
        "RS = SEED\n",
        "\n",
        "if LABEL_COL in df.columns and len(df) > TARGET_N:\n",
        "    counts = df[LABEL_COL].value_counts(dropna=False)\n",
        "    probs = counts / counts.sum()\n",
        "    raw = probs * TARGET_N\n",
        "\n",
        "    base = np.floor(raw).astype(int)                # 기본 할당\n",
        "    leftover = int(TARGET_N - base.sum())           # 남은 몫\n",
        "    remainders = (raw - base).sort_values(ascending=False)\n",
        "\n",
        "    alloc = base.copy()\n",
        "    for lbl in remainders.index[:leftover]:\n",
        "        alloc[lbl] += 1\n",
        "\n",
        "    # 각 그룹에서 할당 수만큼 샘플 (그룹 실제 크기보다 큰 경우 cap)\n",
        "    parts = []\n",
        "    for lbl, n_take in alloc.items():\n",
        "        grp = df[df[LABEL_COL] == lbl]\n",
        "        n_final = min(len(grp), int(n_take))\n",
        "        if n_final > 0:\n",
        "            parts.append(grp.sample(n=n_final, random_state=RS))\n",
        "    df = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RS).reset_index(drop=True)\n",
        "print(df.shape)\n",
        "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
        "\n",
        "# meta 속성(attribute) 조절\n",
        "# meta_cols = [c for c in df.columns if c not in ['text','audio_split_list','urgencyLevel','sentiment']]\n",
        "meta_cols = ['gender', 'disasterLarge', 'disasterMedium']\n",
        "\n",
        "# 분리 전에 전체 데이터로 meta_maps 생성\n",
        "meta_maps = {}\n",
        "for col in meta_cols:\n",
        "    uniques = list(df[col].dropna().unique())\n",
        "    meta_maps[col] = {v: (i + 1) for i, v in enumerate(uniques)} # 0은 padding/unknown 용\n",
        "\n",
        "urgency_map = {'상': 2, '중': 1, '하': 0}\n",
        "sentiment_map = {'불안/걱정':0, '당황/난처':1, '중립':2, '기타부정':3}\n",
        "\n",
        "valid_size = int(len(df) * 0.2)\n",
        "df_train = df.iloc[:-valid_size].reset_index(drop=True)\n",
        "df_valid = df.iloc[-valid_size:].reset_index(drop=True)\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "model, scores = train_model(df_train, df_valid, meta_cols,\n",
        "                            TRAIN_AUDIO_TAR, VALID_AUDIO_TAR,\n",
        "                            urgency_map, sentiment_map,\n",
        "                            meta_maps=meta_maps,\n",
        "                            num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 학습 결과 시각화\n",
        "plot_scores(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aguRDmiuqcQa"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 10. 테스트/추론\n",
        "# =========================================\n",
        "print(\"========================= 테스트 =========================\")\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "df_test = pd.read_csv(VALID_CSV_PATH)\n",
        "\n",
        "# 테스트용 Dataset 생성\n",
        "test_dataset = TextAudioDataset(df_test, meta_cols, VALID_AUDIO_TAR, urgency_map, sentiment_map, meta_maps)\n",
        "\n",
        "# DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# 모델 평가 모드\n",
        "model.eval()\n",
        "\n",
        "# 결과 저장 리스트 초기화\n",
        "preds_urgency, preds_sentiment = [], []\n",
        "labels_urgency, labels_sentiment = [], []\n",
        "\n",
        "# 테스트 루프\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Test\"):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        audio_feat = batch['audio_feat']\n",
        "        meta_idx = batch['meta_idx']\n",
        "        label_urgency = batch['urgency']\n",
        "        label_sentiment = batch['sentiment']\n",
        "\n",
        "        # 예측\n",
        "        pred_urg, pred_sent = model(input_ids, attention_mask, audio_feat, meta_idx)\n",
        "\n",
        "        # 결과 저장\n",
        "        preds_urgency.extend(torch.argmax(pred_urg, dim=1).cpu().tolist())\n",
        "        preds_sentiment.extend(torch.argmax(pred_sent, dim=1).cpu().tolist())\n",
        "        labels_urgency.extend(label_urgency.cpu().tolist())\n",
        "        labels_sentiment.extend(label_sentiment.cpu().tolist())\n",
        "\n",
        "# Urgency 평가 (분류)\n",
        "acc_urg = accuracy_score(labels_urgency, preds_urgency)\n",
        "f1_urg = f1_score(labels_urgency, preds_urgency, average='weighted')\n",
        "\n",
        "# Sentiment 평가 (분류)\n",
        "acc_sent = accuracy_score(labels_sentiment, preds_sentiment)\n",
        "f1_sent = f1_score(labels_sentiment, preds_sentiment, average='weighted')\n",
        "\n",
        "# 출력\n",
        "print(f\"Test Accuracy (urgency): {acc_urg:.4f}\")\n",
        "print(f\"Test Weighted F1 (urgency): {f1_urg:.4f}\")\n",
        "print(f\"Test Accuracy (sentiment): {acc_sent:.4f}\")\n",
        "print(f\"Test Weighted F1 (sentiment): {f1_sent:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxf5z31TDXcM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPhHV7UPDdj8KgywBtUrAoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
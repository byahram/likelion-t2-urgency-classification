{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I_Eh69jOqJT1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAWOKnQeOtz"
      },
      "source": [
        "## 마운트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQesRLXWeMbx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/Projects/LikeLion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-4NAL4gj25V"
      },
      "outputs": [],
      "source": [
        "# TRAIN_CSV_PATH = \"./train/train.csv\"\n",
        "# TRAIN_AUDIO_TAR = \"./train/train_audio.tar\"\n",
        "# VALID_CSV_PATH = \"./validation/validation.csv\"\n",
        "# VALID_AUDIO_TAR = \"./validation/valid_audio.tar\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCn_H5qEeR5s"
      },
      "source": [
        "## 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS08FBd1qcV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import ast\n",
        "from typing import List, Dict\n",
        "import io\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, Wav2Vec2Model, AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================================\n",
        "# 1. 환경 설정 및 랜덤 시드 고정\n",
        "# =========================================\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 1\n",
        "TEXT_MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
        "TRAIN_CSV_PATH = \"./train/train.csv\"\n",
        "TRAIN_AUDIO_TAR = \"./train/train_audio.tar\"\n",
        "VALID_CSV_PATH = \"./validation/validation.csv\"\n",
        "VALID_AUDIO_TAR = \"./validation/valid_audio.tar\"\n",
        "MAX_TEXT_LEN = 128\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_AUDIO_FRAMES = 400\n",
        "MIN_AUDIO_FRAMES = 8\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device : {device}\")\n",
        "\n",
        "# =========================================\n",
        "# 2. 텍스트 전처리 함수\n",
        "# =========================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
        "\n",
        "def preprocess_text(text: str, tokenizer, max_length: int = MAX_TEXT_LEN) -> Dict[str, torch.Tensor]:\n",
        "    if isinstance(text, str) and len(text.strip()) > 0:\n",
        "        sentences = [s.strip().strip('\"') for s in text.split(',') if s.strip()]\n",
        "    else:\n",
        "        sentences = []\n",
        "    joined_text = ' [SEP] '.join(sentences) if sentences else ''\n",
        "    enc = tokenizer(\n",
        "        joined_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return {k: v.squeeze(0) for k, v in enc.items()}\n",
        "\n",
        "# =========================================\n",
        "# 3. wav2vec2-XLSR 모델 초기화\n",
        "# =========================================\n",
        "# wav2vec2-XLSR 모델 초기화\n",
        "W2V_MODEL_NAME = \"facebook/wav2vec2-xls-r-300m\"  # 멀티링구얼 모델 (한국어 포함)\n",
        "w2v_fe = AutoFeatureExtractor.from_pretrained(W2V_MODEL_NAME)\n",
        "w2v_model = Wav2Vec2Model.from_pretrained(W2V_MODEL_NAME).to(device)\n",
        "# w2v_model = Wav2Vec2Model.from_pretrained(\n",
        "#     W2V_MODEL_NAME,\n",
        "#     use_safetensors=True,   # safetensors 우선\n",
        "#     cache_dir=\"./hf_cache\"  # (선택) 캐시 위치 고정\n",
        "# ).to(device)\n",
        "w2v_model.eval()\n",
        "\n",
        "# 여기!\n",
        "for p in w2v_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 샘플링 레이트\n",
        "SAMPLE_RATE = 16000\n",
        "MIN_AUDIO_FRAMES = 8\n",
        "MAX_AUDIO_FRAMES = 400\n",
        "\n",
        "# =========================================\n",
        "# 4. Dataset 정의\n",
        "# =========================================\n",
        "class TextAudioDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, meta_cols: List[str], audio_tar_path: str,\n",
        "                 urgency_map: Dict[str, float], sentiment_map: Dict[str, int]):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.meta_cols = meta_cols\n",
        "        self.audio_tar_path = audio_tar_path\n",
        "        self.urgency_map = urgency_map\n",
        "        self.sentiment_map = sentiment_map\n",
        "\n",
        "        # 메타데이터 인덱스 매핑\n",
        "        self.meta_maps = {}\n",
        "        self.meta_vocab_sizes = {}\n",
        "        for col in meta_cols:\n",
        "            uniques = list(df[col].dropna().unique())\n",
        "            self.meta_maps[col] = {v: (i + 1) for i, v in enumerate(uniques)}\n",
        "            self.meta_vocab_sizes[col] = len(uniques) + 1\n",
        "\n",
        "        # 오디오 로딩 (tar 내부)\n",
        "        self.audio_data = {}\n",
        "        if audio_tar_path and os.path.exists(audio_tar_path):\n",
        "            mode = 'r:gz' if audio_tar_path.endswith(('.tar.gz', '.tgz')) else 'r'\n",
        "            with tarfile.open(audio_tar_path, mode) as tar:\n",
        "                for member in tar.getmembers():\n",
        "                    if member.isfile():\n",
        "                        f = tar.extractfile(member)\n",
        "                        if f:\n",
        "                            self.audio_data[os.path.basename(member.name)] = f.read()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # -----------------------------\n",
        "        # 1) 텍스트 처리\n",
        "        # -----------------------------\n",
        "        text_enc = preprocess_text(row.get('text', ''), tokenizer, MAX_TEXT_LEN)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 2) wav2vec2-XLSR 오디오 처리\n",
        "        # -----------------------------\n",
        "        audio_paths = []\n",
        "        if 'audio_split_list' in row and pd.notna(row['audio_split_list']):\n",
        "            try:\n",
        "                audio_paths = ast.literal_eval(row['audio_split_list'])\n",
        "                if not isinstance(audio_paths, (list, tuple)):\n",
        "                    audio_paths = []\n",
        "            except Exception:\n",
        "                audio_paths = []\n",
        "\n",
        "        features = []\n",
        "        for audio_file in audio_paths:\n",
        "            audio_bytes = self.audio_data.get(audio_file)\n",
        "            if audio_bytes is None:\n",
        "                features.append(None)\n",
        "                continue\n",
        "            try:\n",
        "                waveform, sr = torchaudio.load(io.BytesIO(audio_bytes))  # (ch, T)\n",
        "            except Exception:\n",
        "                features.append(None)\n",
        "                continue\n",
        "\n",
        "            # mono + 16k\n",
        "            if waveform.dim() == 2 and waveform.size(0) > 1:\n",
        "                waveform = waveform.mean(dim=0, keepdim=True)\n",
        "            if sr != SAMPLE_RATE:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fe_out = w2v_fe(\n",
        "                    waveform.squeeze(0).numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\"\n",
        "                )\n",
        "                input_values = fe_out.input_values.to(device)  # (1, T')\n",
        "                out = w2v_model(input_values)                  # (1, t, d)\n",
        "                hs = out.last_hidden_state.squeeze(0)          # (t, d)\n",
        "                hs = hs.transpose(0, 1).contiguous()           # (d, t)\n",
        "\n",
        "                # 프레임 패딩/자르기 (기존 하이퍼 재사용)\n",
        "                frames = hs.size(1)\n",
        "                if frames < MAX_AUDIO_FRAMES:\n",
        "                    hs = torch.nn.functional.pad(hs, (0, MAX_AUDIO_FRAMES - frames))  # ← 무조건 MAX에 맞추기\n",
        "                else:\n",
        "                    hs = hs[:, :MAX_AUDIO_FRAMES]\n",
        "                features.append(hs)\n",
        "\n",
        "        # 여러 조각 평균, 없으면 0-텐서\n",
        "        D = w2v_model.config.hidden_size  # 예: 1024(300m)\n",
        "        if len(features) == 0:\n",
        "            audio_feat = torch.zeros(D, MAX_AUDIO_FRAMES, device=device)\n",
        "        else:\n",
        "            # 조각들 길이 확인\n",
        "            lengths = [f.size(1) for f in features if f is not None]\n",
        "            target_T = MIN_AUDIO_FRAMES if len(lengths) == 0 else min(MAX_AUDIO_FRAMES, max(lengths))\n",
        "\n",
        "            fixed = []\n",
        "            for f in features:\n",
        "                if f is None:\n",
        "                    fixed.append(torch.zeros(D, target_T, device=device))\n",
        "                    continue\n",
        "                t = f.size(1)\n",
        "                if t < target_T:\n",
        "                    f = torch.nn.functional.pad(f, (0, target_T - t))\n",
        "                elif t > target_T:\n",
        "                    f = f[:, :target_T]\n",
        "                fixed.append(f)\n",
        "\n",
        "            audio_feat = torch.stack(fixed, dim=0).mean(dim=0)  # (D, target_T)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 3) 메타데이터 처리\n",
        "        # -----------------------------\n",
        "        meta_idx = []\n",
        "        for col in self.meta_cols:\n",
        "            val = row.get(col, None)\n",
        "            idx_val = 0 if pd.isna(val) or val is None else self.meta_maps.get(col, {}).get(val, 0)\n",
        "            vocab_size = self.meta_vocab_sizes.get(col, 1)\n",
        "            idx_val = max(0, min(idx_val, vocab_size-1))\n",
        "            meta_idx.append(idx_val)\n",
        "        meta_idx = torch.tensor(meta_idx, dtype=torch.long) if meta_idx else torch.tensor([0], dtype=torch.long)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 4) Labels 처리\n",
        "        # -----------------------------\n",
        "        urg_val = row.get('urgencyLevel', None)\n",
        "        urgency = self.urgency_map.get(urg_val, self.urgency_map.get(str(urg_val), 0.0))\n",
        "        try:\n",
        "            urgency = float(urgency)\n",
        "        except Exception:\n",
        "            urgency = 0.0\n",
        "        urgency = torch.tensor(urgency, dtype=torch.float)\n",
        "\n",
        "        sent_val = row.get('sentiment', None)\n",
        "        sentiment_idx = self.sentiment_map.get(sent_val, self.sentiment_map.get(str(sent_val), 0))\n",
        "        num_classes = max(1, len(self.sentiment_map))\n",
        "        sentiment_idx = max(0, min(sentiment_idx, num_classes-1))\n",
        "        sentiment = torch.tensor(sentiment_idx, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': text_enc['input_ids'].to(device),\n",
        "            'attention_mask': text_enc['attention_mask'].to(device),\n",
        "            'audio_feat': audio_feat.to(device),\n",
        "            'meta_idx': meta_idx.to(device),\n",
        "            'urgency': urgency.to(device),\n",
        "            'sentiment': sentiment.to(device)\n",
        "        }\n",
        "\n",
        "# =========================================\n",
        "# 3-1. collate_fn: dynamic padding\n",
        "# =========================================\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    meta_idx = torch.stack([item['meta_idx'] for item in batch])\n",
        "    urgency = torch.stack([item['urgency'] for item in batch])\n",
        "    sentiment = torch.stack([item['sentiment'] for item in batch])\n",
        "\n",
        "    audio_feats = [item['audio_feat'] for item in batch]\n",
        "    max_frames = max([feat.size(1) for feat in audio_feats])\n",
        "    padded_audio = [torch.nn.functional.pad(feat, (0, max_frames - feat.size(1))) for feat in audio_feats]\n",
        "    audio_feat = torch.stack(padded_audio)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'audio_feat': audio_feat,\n",
        "        'meta_idx': meta_idx,\n",
        "        'urgency': urgency,\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "# =========================================\n",
        "# 4. 모델 정의 (wav2vec2-XLSR용)\n",
        "# =========================================\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_model_name, meta_maps: Dict[str, Dict],\n",
        "                 audio_emb_dim=256, joint_dim=256, num_classes_sentiment=4,\n",
        "                 audio_input_dim=None):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
        "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, joint_dim)\n",
        "\n",
        "        if audio_input_dim is None:\n",
        "            audio_input_dim = w2v_model.config.hidden_size  # wav2vec2-XLSR의 hidden dim\n",
        "\n",
        "        self.audio_proj_shared   = nn.Linear(audio_input_dim, audio_emb_dim)\n",
        "        self.audio_proj_urgency  = nn.Linear(audio_emb_dim, joint_dim)\n",
        "        self.audio_proj_sentiment= nn.Linear(audio_emb_dim, joint_dim)\n",
        "\n",
        "        self.meta_embs = nn.ModuleDict()\n",
        "        for k, mapping in meta_maps.items():\n",
        "            num_embeddings = len(mapping) + 1\n",
        "            emb_dim = min(8, num_embeddings)\n",
        "            self.meta_embs[k] = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=emb_dim, padding_idx=0)\n",
        "        self.meta_out_dim = sum([emb.embedding_dim for emb in self.meta_embs.values()]) if self.meta_embs else 0\n",
        "\n",
        "        self.urgency_head = nn.Linear(joint_dim + self.meta_out_dim, 1)\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(joint_dim + self.meta_out_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_sentiment)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_feat, meta_idx):\n",
        "        B = input_ids.size(0)\n",
        "        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_emb = self.text_proj(txt_out.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # audio_feat: (B, D, T)\n",
        "        audio_mean = audio_feat.mean(dim=2)  # (B, D)\n",
        "        audio_emb = self.audio_proj_shared(audio_mean)\n",
        "        audio_emb_urgency  = self.audio_proj_urgency(audio_emb)\n",
        "        audio_emb_sentiment= self.audio_proj_sentiment(audio_emb)\n",
        "\n",
        "        if not self.meta_embs:\n",
        "            meta_emb = torch.zeros(B, 0, device=input_ids.device)\n",
        "        else:\n",
        "            if meta_idx.dim() == 1:\n",
        "                meta_idx = meta_idx.unsqueeze(0).expand(B, -1)\n",
        "            meta_embeddings = [self.meta_embs[k](meta_idx[:, i].clamp(0, self.meta_embs[k].num_embeddings-1))\n",
        "                               for i, k in enumerate(self.meta_embs.keys())]\n",
        "            meta_emb = torch.cat(meta_embeddings, dim=1)\n",
        "\n",
        "        joint_urgency   = txt_emb + audio_emb_urgency\n",
        "        joint_sentiment = txt_emb + audio_emb_sentiment\n",
        "\n",
        "        urg_out  = self.urgency_head(torch.cat([joint_urgency, meta_emb], dim=1)).squeeze(1)\n",
        "        sent_out = self.sentiment_head(torch.cat([joint_sentiment, meta_emb], dim=1))\n",
        "        return urg_out, sent_out\n",
        "\n",
        "# =========================================\n",
        "# 5. 학습 함수\n",
        "# =========================================\n",
        "def train_model(df_train, df_valid, meta_cols, train_tar, valid_tar,\n",
        "                urgency_map, sentiment_map,\n",
        "                num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE):\n",
        "\n",
        "    meta_maps = {col: {v: (i + 1) for i, v in enumerate(df_train[col].dropna().unique())} for col in meta_cols}\n",
        "    train_dataset = TextAudioDataset(df_train, meta_cols, train_tar, urgency_map, sentiment_map)\n",
        "    valid_dataset = TextAudioDataset(df_valid, meta_cols, valid_tar, urgency_map, sentiment_map)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = MultimodalClassifier(\n",
        "        TEXT_MODEL_NAME, meta_maps,\n",
        "        num_classes_sentiment=len(sentiment_map),\n",
        "        audio_input_dim=w2v_model.config.hidden_size  # wav2vec2-XLSR의 hidden dim\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    criterion_reg = nn.MSELoss()\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses, train_reg_losses, train_cls_losses = [], [], []\n",
        "    valid_losses, valid_reg_losses, valid_cls_losses = [], [], []\n",
        "    mse_list, mae_list, acc_list, f1_list = [], [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss, total_train_reg, total_train_cls = 0, 0, 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            pred_urg, pred_sent = model(batch['input_ids'], batch['attention_mask'],\n",
        "                                        batch['audio_feat'], batch['meta_idx'])\n",
        "            reg_loss = criterion_reg(pred_urg, batch['urgency'])\n",
        "            cls_loss = criterion_cls(pred_sent, batch['sentiment'])\n",
        "            loss = reg_loss + cls_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_reg += reg_loss.item()\n",
        "            total_train_cls += cls_loss.item()\n",
        "\n",
        "        train_losses.append(total_train_loss / len(train_loader))\n",
        "        train_reg_losses.append(total_train_reg / len(train_loader))\n",
        "        train_cls_losses.append(total_train_cls / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        total_valid_loss, total_valid_reg, total_valid_cls = 0, 0, 0\n",
        "        all_urg_pred, all_urg_true = [], []\n",
        "        all_sent_pred, all_sent_true = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1} [Valid]\"):\n",
        "                pred_urg, pred_sent = model(batch['input_ids'], batch['attention_mask'],\n",
        "                                            batch['audio_feat'], batch['meta_idx'])\n",
        "                reg_loss = criterion_reg(pred_urg, batch['urgency'])\n",
        "                cls_loss = criterion_cls(pred_sent, batch['sentiment'])\n",
        "                loss = reg_loss + cls_loss\n",
        "\n",
        "                total_valid_loss += loss.item()\n",
        "                total_valid_reg += reg_loss.item()\n",
        "                total_valid_cls += cls_loss.item()\n",
        "\n",
        "                all_urg_pred.extend(pred_urg.cpu().numpy())\n",
        "                all_urg_true.extend(batch['urgency'].cpu().numpy())\n",
        "                all_sent_pred.extend(pred_sent.argmax(dim=1).cpu().numpy())\n",
        "                all_sent_true.extend(batch['sentiment'].cpu().numpy())\n",
        "\n",
        "        valid_losses.append(total_valid_loss / len(valid_loader))\n",
        "        valid_reg_losses.append(total_valid_reg / len(valid_loader))\n",
        "        valid_cls_losses.append(total_valid_cls / len(valid_loader))\n",
        "\n",
        "        mse = mean_squared_error(all_urg_true, all_urg_pred)\n",
        "        mae = mean_absolute_error(all_urg_true, all_urg_pred)\n",
        "        acc = accuracy_score(all_sent_true, all_sent_pred)\n",
        "        f1 = f1_score(all_sent_true, all_sent_pred, average='weighted')\n",
        "\n",
        "        mse_list.append(mse)\n",
        "        mae_list.append(mae)\n",
        "        acc_list.append(acc)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_losses[-1]:.4f} | Valid Loss: {valid_losses[-1]:.4f}\")\n",
        "        print(f\"Train Reg Loss: {train_reg_losses[-1]:.4f} | Train Cls Loss: {train_cls_losses[-1]:.4f}\")\n",
        "        print(f\"Valid Reg Loss: {valid_reg_losses[-1]:.4f} | Valid Cls Loss: {valid_cls_losses[-1]:.4f}\")\n",
        "        print(f\"Regression -> MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
        "        print(f\"Classification -> Accuracy: {acc:.4f}, Weighted F1: {f1:.4f}\\n\")\n",
        "\n",
        "    scores = {\n",
        "        'train_loss': train_losses, 'train_reg_loss': train_reg_losses, 'train_cls_loss': train_cls_losses,\n",
        "        'valid_loss': valid_losses, 'valid_reg_loss': valid_reg_losses, 'valid_cls_loss': valid_cls_losses,\n",
        "        'mse': mse_list, 'mae': mae_list, 'accuracy': acc_list, 'f1': f1_list\n",
        "    }\n",
        "    return model, scores\n",
        "\n",
        "# =========================================\n",
        "# 6. 학습 결과 시각화\n",
        "# =========================================\n",
        "def plot_scores(scores):\n",
        "    epochs = range(1, len(scores['train_loss'])+1)\n",
        "    plt.figure(figsize=(14,6))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, scores['train_reg_loss'], label='Train Reg Loss')\n",
        "    plt.plot(epochs, scores['train_cls_loss'], label='Train Cls Loss')\n",
        "    plt.plot(epochs, scores['valid_reg_loss'], '--', label='Valid Reg Loss')\n",
        "    plt.plot(epochs, scores['valid_cls_loss'], '--', label='Valid Cls Loss')\n",
        "    plt.title('Regression & Classification Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, scores['mse'], label='MSE')\n",
        "    plt.plot(epochs, scores['mae'], label='MAE')\n",
        "    plt.plot(epochs, scores['accuracy'], label='Accuracy')\n",
        "    plt.plot(epochs, scores['f1'], label='F1 Score')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0_MLlnQqcS7"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 7. 학습 실행\n",
        "# =========================================\n",
        "df = pd.read_csv(TRAIN_CSV_PATH)\n",
        "\n",
        "# [아람EDITED] - disasterLarge 비율 유지하여 약 500개로 다운샘플\n",
        "LABEL_COL = \"disasterLarge\"  # 비율 기준 컬럼명\n",
        "TARGET_N = 500               # 다운샘플 목표 개수\n",
        "RS = SEED\n",
        "\n",
        "if LABEL_COL in df.columns and len(df) > TARGET_N:\n",
        "    # --- Stratified downsample (Largest Remainder Method) ---\n",
        "    counts = df[LABEL_COL].value_counts(dropna=False)\n",
        "    probs = counts / counts.sum()\n",
        "    raw = probs * TARGET_N\n",
        "\n",
        "    base = np.floor(raw).astype(int)                # 기본 할당\n",
        "    leftover = int(TARGET_N - base.sum())           # 남은 몫\n",
        "    # 가장 큰 소수점부터 1개씩 추가\n",
        "    remainders = (raw - base).sort_values(ascending=False)\n",
        "\n",
        "    alloc = base.copy()\n",
        "    for lbl in remainders.index[:leftover]:\n",
        "        alloc[lbl] += 1\n",
        "\n",
        "    # 각 그룹에서 할당 수만큼 샘플 (그룹 실제 크기보다 큰 경우 cap)\n",
        "    parts = []\n",
        "    for lbl, n_take in alloc.items():\n",
        "        grp = df[df[LABEL_COL] == lbl]\n",
        "        n_final = min(len(grp), int(n_take))\n",
        "        if n_final > 0:\n",
        "            parts.append(grp.sample(n=n_final, random_state=RS))\n",
        "    df = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RS).reset_index(drop=True)\n",
        "print(df.shape)\n",
        "# [아람EDITED]\n",
        "\n",
        "meta_cols = [c for c in df.columns if c not in ['text','audio_split_list','urgencyLevel','sentiment']]\n",
        "\n",
        "urgency_map = {'상': 2.0, '중': 1.0, '하': 0.0}\n",
        "sentiment_map = {'불안/걱정':0, '당황/난처':1, '중립':2, '기타부정':3}\n",
        "\n",
        "valid_size = int(len(df) * 0.2)\n",
        "df_train = df.iloc[:-valid_size].reset_index(drop=True)\n",
        "df_valid = df.iloc[-valid_size:].reset_index(drop=True)\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "model, scores = train_model(df_train, df_valid, meta_cols,\n",
        "                            TRAIN_AUDIO_TAR, VALID_AUDIO_TAR,\n",
        "                            urgency_map, sentiment_map)\n",
        "\n",
        "# 학습 결과 시각화\n",
        "plot_scores(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aguRDmiuqcQa"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 10. 테스트 평가\n",
        "# =========================================\n",
        "print(\"========================= 테스트 =========================\")\n",
        "df_test = pd.read_csv(VALID_CSV_PATH)\n",
        "\n",
        "test_dataset = TextAudioDataset(df_test, meta_cols, VALID_AUDIO_TAR, urgency_map, sentiment_map)\n",
        "\n",
        "# [수정] num_workers=0 으로 변경, pin_memory=False (기본값)\n",
        "# Dataset의 __getitem__에서 이미 .to(device)로 GPU에 텐서를 올리고 있으므로,\n",
        "# 멀티프로세싱(num_workers>0)을 사용하면 CUDA 에러가 발생합니다.\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False,\n",
        "                         collate_fn=collate_fn, # collate_fn 추가\n",
        "                         num_workers=0,      # 수정\n",
        "                         pin_memory=False)   # 수정\n",
        "\n",
        "model.eval()\n",
        "preds_urgency, preds_sentiment, labels_urgency, labels_sentiment = [], [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Test\"):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        audio_feat = batch['audio_feat']\n",
        "        meta_idx = batch['meta_idx']\n",
        "        label_urgency = batch['urgency']\n",
        "        label_sentiment = batch['sentiment']\n",
        "\n",
        "        pred_urgency, pred_sentiment = model(input_ids, attention_mask, audio_feat, meta_idx)\n",
        "\n",
        "        preds_urgency.extend(pred_urgency.cpu().tolist())\n",
        "        preds_sentiment.extend(torch.argmax(pred_sentiment, dim=1).cpu().tolist())\n",
        "        labels_urgency.extend(label_urgency.cpu().tolist())\n",
        "        labels_sentiment.extend(label_sentiment.cpu().tolist())\n",
        "\n",
        "mae = mean_absolute_error(labels_urgency, preds_urgency)\n",
        "acc = accuracy_score(labels_sentiment, preds_sentiment)\n",
        "f1 = f1_score(labels_sentiment, preds_sentiment, average='weighted') # F1 스코어 추가\n",
        "\n",
        "print(f\"Test MAE (urgencyLevel): {mae:.4f}\")\n",
        "print(f\"Test Accuracy (sentiment): {acc:.4f}\")\n",
        "print(f\"Test Weighted F1 (sentiment): {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHl34NsAHhIy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN6cMikWnjWwyfEp7V621Ip"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "%cd /gdrive/MyDrive/likelion/project1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import ast\n",
    "from typing import List, Dict\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2Model, AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================\n",
    "# 1. 환경 설정 및 랜덤 시드 고정\n",
    "# =========================================\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "TEXT_MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
    "TRAIN_CSV_PATH = \"./data/training/train.csv\"\n",
    "TRAIN_AUDIO_TAR = \"./data/training/train_audio.tar\"\n",
    "VALID_CSV_PATH = \"./data/validation/valid.csv\"\n",
    "VALID_AUDIO_TAR = \"./data/validation/total/valid_audio.tar\"\n",
    "MAX_TEXT_LEN = 128\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_FRAMES = 400\n",
    "MIN_AUDIO_FRAMES = 8\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "# =========================================\n",
    "# 2. 텍스트 전처리 함수\n",
    "# =========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "def preprocess_text(text: str, tokenizer, max_length: int = MAX_TEXT_LEN) -> Dict[str, torch.Tensor]:\n",
    "    if isinstance(text, str) and len(text.strip()) > 0:\n",
    "        sentences = [s.strip().strip('\"') for s in text.split(',') if s.strip()]\n",
    "    else:\n",
    "        sentences = []\n",
    "    joined_text = ' [SEP] '.join(sentences) if sentences else ''\n",
    "    enc = tokenizer(\n",
    "        joined_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "# =========================================\n",
    "# 3. wav2vec2-XLSR 모델 초기화\n",
    "# =========================================\n",
    "# wav2vec2-XLSR 모델 초기화\n",
    "W2V_MODEL_NAME = \"facebook/wav2vec2-xls-r-300m\"  # 멀티링구얼 모델 (한국어 포함)\n",
    "w2v_fe = AutoFeatureExtractor.from_pretrained(W2V_MODEL_NAME)\n",
    "w2v_model = Wav2Vec2Model.from_pretrained(W2V_MODEL_NAME).to(device)\n",
    "# w2v_model = Wav2Vec2Model.from_pretrained(\n",
    "#     W2V_MODEL_NAME,\n",
    "#     use_safetensors=True,   # safetensors 우선\n",
    "#     cache_dir=\"./hf_cache\"  # (선택) 캐시 위치 고정\n",
    "# ).to(device)\n",
    "w2v_model.eval()\n",
    "\n",
    "# 여기!\n",
    "for p in w2v_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 샘플링 레이트\n",
    "SAMPLE_RATE = 16000\n",
    "MIN_AUDIO_FRAMES = 8\n",
    "MAX_AUDIO_FRAMES = 400\n",
    "\n",
    "# =========================================\n",
    "# 4. Dataset 정의\n",
    "# =========================================\n",
    "class TextAudioDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, meta_cols: List[str], audio_tar_path: str,\n",
    "                 urgency_map: Dict[str, float], sentiment_map: Dict[str, int]):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.meta_cols = meta_cols\n",
    "        self.audio_tar_path = audio_tar_path\n",
    "        self.urgency_map = urgency_map\n",
    "        self.sentiment_map = sentiment_map\n",
    "\n",
    "        # 메타데이터 인덱스 매핑\n",
    "        self.meta_maps = {}\n",
    "        self.meta_vocab_sizes = {}\n",
    "        for col in meta_cols:\n",
    "            uniques = list(df[col].dropna().unique())\n",
    "            self.meta_maps[col] = {v: (i + 1) for i, v in enumerate(uniques)}\n",
    "            self.meta_vocab_sizes[col] = len(uniques) + 1\n",
    "\n",
    "        # 오디오 로딩 (tar 내부)\n",
    "        self.audio_data = {}\n",
    "        if audio_tar_path and os.path.exists(audio_tar_path):\n",
    "            mode = 'r:gz' if audio_tar_path.endswith(('.tar.gz', '.tgz')) else 'r'\n",
    "            with tarfile.open(audio_tar_path, mode) as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    if member.isfile():\n",
    "                        f = tar.extractfile(member)\n",
    "                        if f:\n",
    "                            self.audio_data[os.path.basename(member.name)] = f.read()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1) 텍스트 처리\n",
    "        # -----------------------------\n",
    "        text_enc = preprocess_text(row.get('text', ''), tokenizer, MAX_TEXT_LEN)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2) wav2vec2-XLSR 오디오 처리\n",
    "        # -----------------------------\n",
    "        audio_paths = []\n",
    "        if 'audio_split_list' in row and pd.notna(row['audio_split_list']):\n",
    "            try:\n",
    "                audio_paths = ast.literal_eval(row['audio_split_list'])\n",
    "                if not isinstance(audio_paths, (list, tuple)):\n",
    "                    audio_paths = []\n",
    "            except Exception:\n",
    "                audio_paths = []\n",
    "\n",
    "        features = []\n",
    "        for audio_file in audio_paths:\n",
    "            audio_bytes = self.audio_data.get(audio_file)\n",
    "            if audio_bytes is None:\n",
    "                features.append(None)\n",
    "                continue\n",
    "            try:\n",
    "                waveform, sr = torchaudio.load(io.BytesIO(audio_bytes))  # (ch, T)\n",
    "            except Exception:\n",
    "                features.append(None)\n",
    "                continue\n",
    "\n",
    "            # mono + 16k\n",
    "            if waveform.dim() == 2 and waveform.size(0) > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            if sr != SAMPLE_RATE:\n",
    "                waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fe_out = w2v_fe(\n",
    "                    waveform.squeeze(0).numpy(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\"\n",
    "                )\n",
    "                input_values = fe_out.input_values.to(device)  # (1, T')\n",
    "                out = w2v_model(input_values)                  # (1, t, d)\n",
    "                hs = out.last_hidden_state.squeeze(0)          # (t, d)\n",
    "                hs = hs.transpose(0, 1).contiguous()           # (d, t)\n",
    "\n",
    "                # 프레임 패딩/자르기 (기존 하이퍼 재사용)\n",
    "                frames = hs.size(1)\n",
    "                if frames < MAX_AUDIO_FRAMES:\n",
    "                    hs = torch.nn.functional.pad(hs, (0, MAX_AUDIO_FRAMES - frames))  # ← 무조건 MAX에 맞추기\n",
    "                else:\n",
    "                    hs = hs[:, :MAX_AUDIO_FRAMES]\n",
    "                features.append(hs)\n",
    "\n",
    "        # 여러 조각 평균, 없으면 0-텐서\n",
    "        D = w2v_model.config.hidden_size  # 예: 1024(300m)\n",
    "        if len(features) == 0:\n",
    "            audio_feat = torch.zeros(D, MAX_AUDIO_FRAMES, device=device)\n",
    "        else:\n",
    "            # 조각들 길이 확인\n",
    "            lengths = [f.size(1) for f in features if f is not None]\n",
    "            target_T = MIN_AUDIO_FRAMES if len(lengths) == 0 else min(MAX_AUDIO_FRAMES, max(lengths))\n",
    "\n",
    "            fixed = []\n",
    "            for f in features:\n",
    "                if f is None:\n",
    "                    fixed.append(torch.zeros(D, target_T, device=device))\n",
    "                    continue\n",
    "                t = f.size(1)\n",
    "                if t < target_T:\n",
    "                    f = torch.nn.functional.pad(f, (0, target_T - t))\n",
    "                elif t > target_T:\n",
    "                    f = f[:, :target_T]\n",
    "                fixed.append(f)\n",
    "\n",
    "            audio_feat = torch.stack(fixed, dim=0).mean(dim=0)  # (D, target_T)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3) 메타데이터 처리\n",
    "        # -----------------------------\n",
    "        meta_idx = []\n",
    "        for col in self.meta_cols:\n",
    "            val = row.get(col, None)\n",
    "            idx_val = 0 if pd.isna(val) or val is None else self.meta_maps.get(col, {}).get(val, 0)\n",
    "            vocab_size = self.meta_vocab_sizes.get(col, 1)\n",
    "            idx_val = max(0, min(idx_val, vocab_size-1))\n",
    "            meta_idx.append(idx_val)\n",
    "        meta_idx = torch.tensor(meta_idx, dtype=torch.long) if meta_idx else torch.tensor([0], dtype=torch.long)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4) Labels 처리\n",
    "        # -----------------------------\n",
    "        urg_val = row.get('urgencyLevel', None)\n",
    "        urgency = self.urgency_map.get(urg_val, self.urgency_map.get(str(urg_val), 0.0))\n",
    "        try:\n",
    "            urgency = float(urgency)\n",
    "        except Exception:\n",
    "            urgency = 0.0\n",
    "        urgency = torch.tensor(urgency, dtype=torch.float)\n",
    "\n",
    "        sent_val = row.get('sentiment', None)\n",
    "        sentiment_idx = self.sentiment_map.get(sent_val, self.sentiment_map.get(str(sent_val), 0))\n",
    "        num_classes = max(1, len(self.sentiment_map))\n",
    "        sentiment_idx = max(0, min(sentiment_idx, num_classes-1))\n",
    "        sentiment = torch.tensor(sentiment_idx, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': text_enc['input_ids'].to(device),\n",
    "            'attention_mask': text_enc['attention_mask'].to(device),\n",
    "            'audio_feat': audio_feat.to(device),\n",
    "            'meta_idx': meta_idx.to(device),\n",
    "            'urgency': urgency.to(device),\n",
    "            'sentiment': sentiment.to(device)\n",
    "        }\n",
    "\n",
    "# =========================================\n",
    "# 3-1. collate_fn: dynamic padding\n",
    "# =========================================\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    meta_idx = torch.stack([item['meta_idx'] for item in batch])\n",
    "    urgency = torch.stack([item['urgency'] for item in batch])\n",
    "    sentiment = torch.stack([item['sentiment'] for item in batch])\n",
    "\n",
    "    audio_feats = [item['audio_feat'] for item in batch]\n",
    "    max_frames = max([feat.size(1) for feat in audio_feats])\n",
    "    padded_audio = [torch.nn.functional.pad(feat, (0, max_frames - feat.size(1))) for feat in audio_feats]\n",
    "    audio_feat = torch.stack(padded_audio)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'audio_feat': audio_feat,\n",
    "        'meta_idx': meta_idx,\n",
    "        'urgency': urgency,\n",
    "        'sentiment': sentiment\n",
    "    }\n",
    "\n",
    "# =========================================\n",
    "# 4. 모델 정의 (wav2vec2-XLSR용)\n",
    "# =========================================\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, text_model_name, meta_maps: Dict[str, Dict],\n",
    "                 audio_emb_dim=256, joint_dim=256, num_classes_sentiment=4,\n",
    "                 audio_input_dim=None):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, joint_dim)\n",
    "\n",
    "        if audio_input_dim is None:\n",
    "            audio_input_dim = w2v_model.config.hidden_size\n",
    "\n",
    "        self.audio_proj_shared = nn.Linear(audio_input_dim, audio_emb_dim)\n",
    "        self.audio_proj_urgency = nn.Linear(audio_emb_dim, joint_dim)\n",
    "        self.audio_proj_sentiment = nn.Linear(audio_emb_dim, joint_dim)\n",
    "\n",
    "        # --- Meta Embeddings ---\n",
    "        self.meta_embs = nn.ModuleDict()\n",
    "        for k, mapping in meta_maps.items():\n",
    "            num_embeddings = len(mapping) + 1\n",
    "            emb_dim = min(8, num_embeddings)\n",
    "            self.meta_embs[k] = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=emb_dim, padding_idx=0)\n",
    "        self.meta_out_dim = sum([emb.embedding_dim for emb in self.meta_embs.values()]) if self.meta_embs else 0\n",
    "\n",
    "        # --- 최종 Fusion MLP (text + audio + meta) ---\n",
    "        self.fusion_dim = joint_dim * 2 + self.meta_out_dim\n",
    "        self.fusion_mlp_urgency = nn.Sequential(\n",
    "            nn.Linear(self.fusion_dim, joint_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.fusion_mlp_sentiment = nn.Sequential(\n",
    "            nn.Linear(self.fusion_dim, joint_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # --- Output Heads ---\n",
    "        self.urgency_head = nn.Linear(joint_dim, 1)\n",
    "        self.sentiment_head = nn.Linear(joint_dim, num_classes_sentiment)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, audio_feat, meta_idx):\n",
    "        B = input_ids.size(0)\n",
    "        # --- Text Encoder ---\n",
    "        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        txt_emb = self.text_proj(txt_out.last_hidden_state[:, 0, :])\n",
    "\n",
    "        # --- Audio Encoder ---\n",
    "        audio_mean = audio_feat.mean(dim=2)  # (B, D)\n",
    "        audio_emb = self.audio_proj_shared(audio_mean)\n",
    "        audio_emb_urgency = self.audio_proj_urgency(audio_emb)\n",
    "        audio_emb_sentiment = self.audio_proj_sentiment(audio_emb)\n",
    "\n",
    "        # --- Meta Embedding ---\n",
    "        if not self.meta_embs:\n",
    "            meta_emb = torch.zeros(B, 0, device=input_ids.device)\n",
    "        else:\n",
    "            if meta_idx.dim() == 1:\n",
    "                meta_idx = meta_idx.unsqueeze(0).expand(B, -1)\n",
    "            meta_embeddings = [self.meta_embs[k](meta_idx[:, i].clamp(0, self.meta_embs[k].num_embeddings-1))\n",
    "                               for i, k in enumerate(self.meta_embs.keys())]\n",
    "            meta_emb = torch.cat(meta_embeddings, dim=1)\n",
    "\n",
    "        # --- Fusion: concat text + audio + meta ---\n",
    "        fusion_urgency = torch.cat([txt_emb, audio_emb_urgency, meta_emb], dim=1)\n",
    "        fusion_sentiment = torch.cat([txt_emb, audio_emb_sentiment, meta_emb], dim=1)\n",
    "\n",
    "        # --- Fusion MLP 통과 ---\n",
    "        joint_urgency = self.fusion_mlp_urgency(fusion_urgency)\n",
    "        joint_sentiment = self.fusion_mlp_sentiment(fusion_sentiment)\n",
    "\n",
    "        # --- Output Heads ---\n",
    "        urg_out = self.urgency_head(joint_urgency).squeeze(1)\n",
    "        sent_out = self.sentiment_head(joint_sentiment)\n",
    "\n",
    "        return urg_out, sent_out\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 5. 학습 함수\n",
    "# =========================================\n",
    "def train_model(df_train, df_valid, meta_cols, train_tar, valid_tar,\n",
    "                urgency_map, sentiment_map,\n",
    "                num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE):\n",
    "\n",
    "    meta_maps = {col: {v: (i + 1) for i, v in enumerate(df_train[col].dropna().unique())} for col in meta_cols}\n",
    "    train_dataset = TextAudioDataset(df_train, meta_cols, train_tar, urgency_map, sentiment_map)\n",
    "    valid_dataset = TextAudioDataset(df_valid, meta_cols, valid_tar, urgency_map, sentiment_map)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = MultimodalClassifier(\n",
    "        TEXT_MODEL_NAME, meta_maps,\n",
    "        num_classes_sentiment=len(sentiment_map),\n",
    "        audio_input_dim=w2v_model.config.hidden_size  # wav2vec2-XLSR의 hidden dim\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion_reg = nn.MSELoss()\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, train_reg_losses, train_cls_losses = [], [], []\n",
    "    valid_losses, valid_reg_losses, valid_cls_losses = [], [], []\n",
    "    mse_list, mae_list, acc_list, f1_list = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_train_reg, total_train_cls = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            optimizer.zero_grad()\n",
    "            pred_urg, pred_sent = model(batch['input_ids'], batch['attention_mask'],\n",
    "                                        batch['audio_feat'], batch['meta_idx'])\n",
    "            reg_loss = criterion_reg(pred_urg, batch['urgency'])\n",
    "            cls_loss = criterion_cls(pred_sent, batch['sentiment'])\n",
    "            loss = reg_loss + cls_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_reg += reg_loss.item()\n",
    "            total_train_cls += cls_loss.item()\n",
    "\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "        train_reg_losses.append(total_train_reg / len(train_loader))\n",
    "        train_cls_losses.append(total_train_cls / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        total_valid_loss, total_valid_reg, total_valid_cls = 0, 0, 0\n",
    "        all_urg_pred, all_urg_true = [], []\n",
    "        all_sent_pred, all_sent_true = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1} [Valid]\"):\n",
    "                pred_urg, pred_sent = model(batch['input_ids'], batch['attention_mask'],\n",
    "                                            batch['audio_feat'], batch['meta_idx'])\n",
    "                reg_loss = criterion_reg(pred_urg, batch['urgency'])\n",
    "                cls_loss = criterion_cls(pred_sent, batch['sentiment'])\n",
    "                loss = reg_loss + cls_loss\n",
    "\n",
    "                total_valid_loss += loss.item()\n",
    "                total_valid_reg += reg_loss.item()\n",
    "                total_valid_cls += cls_loss.item()\n",
    "\n",
    "                all_urg_pred.extend(pred_urg.cpu().numpy())\n",
    "                all_urg_true.extend(batch['urgency'].cpu().numpy())\n",
    "                all_sent_pred.extend(pred_sent.argmax(dim=1).cpu().numpy())\n",
    "                all_sent_true.extend(batch['sentiment'].cpu().numpy())\n",
    "\n",
    "        valid_losses.append(total_valid_loss / len(valid_loader))\n",
    "        valid_reg_losses.append(total_valid_reg / len(valid_loader))\n",
    "        valid_cls_losses.append(total_valid_cls / len(valid_loader))\n",
    "\n",
    "        mse = mean_squared_error(all_urg_true, all_urg_pred)\n",
    "        mae = mean_absolute_error(all_urg_true, all_urg_pred)\n",
    "        acc = accuracy_score(all_sent_true, all_sent_pred)\n",
    "        f1 = f1_score(all_sent_true, all_sent_pred, average='weighted')\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        mae_list.append(mae)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_losses[-1]:.4f} | Valid Loss: {valid_losses[-1]:.4f}\")\n",
    "        print(f\"Train Reg Loss: {train_reg_losses[-1]:.4f} | Train Cls Loss: {train_cls_losses[-1]:.4f}\")\n",
    "        print(f\"Valid Reg Loss: {valid_reg_losses[-1]:.4f} | Valid Cls Loss: {valid_cls_losses[-1]:.4f}\")\n",
    "        print(f\"Regression -> MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "        print(f\"Classification -> Accuracy: {acc:.4f}, Weighted F1: {f1:.4f}\\n\")\n",
    "\n",
    "    scores = {\n",
    "        'train_loss': train_losses, 'train_reg_loss': train_reg_losses, 'train_cls_loss': train_cls_losses,\n",
    "        'valid_loss': valid_losses, 'valid_reg_loss': valid_reg_losses, 'valid_cls_loss': valid_cls_losses,\n",
    "        'mse': mse_list, 'mae': mae_list, 'accuracy': acc_list, 'f1': f1_list\n",
    "    }\n",
    "    return model, scores\n",
    "\n",
    "# =========================================\n",
    "# 6. 학습 결과 시각화\n",
    "# =========================================\n",
    "def plot_scores(scores):\n",
    "    epochs = range(1, len(scores['train_loss'])+1)\n",
    "    plt.figure(figsize=(14,6))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, scores['train_reg_loss'], label='Train Reg Loss')\n",
    "    plt.plot(epochs, scores['train_cls_loss'], label='Train Cls Loss')\n",
    "    plt.plot(epochs, scores['valid_reg_loss'], '--', label='Valid Reg Loss')\n",
    "    plt.plot(epochs, scores['valid_cls_loss'], '--', label='Valid Cls Loss')\n",
    "    plt.title('Regression & Classification Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, scores['mse'], label='MSE')\n",
    "    plt.plot(epochs, scores['mae'], label='MAE')\n",
    "    plt.plot(epochs, scores['accuracy'], label='Accuracy')\n",
    "    plt.plot(epochs, scores['f1'], label='F1 Score')\n",
    "    plt.title('Validation Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 7. 학습 실행\n",
    "# =========================================\n",
    "df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# [아람EDITED] - disasterLarge 비율 유지하여 약 500개로 다운샘플\n",
    "LABEL_COL = \"disasterLarge\"  # 비율 기준 컬럼명\n",
    "TARGET_N = 500               # 다운샘플 목표 개수\n",
    "RS = SEED\n",
    "\n",
    "if LABEL_COL in df.columns and len(df) > TARGET_N:\n",
    "    # --- Stratified downsample (Largest Remainder Method) ---\n",
    "    counts = df[LABEL_COL].value_counts(dropna=False)\n",
    "    probs = counts / counts.sum()\n",
    "    raw = probs * TARGET_N\n",
    "\n",
    "    base = np.floor(raw).astype(int)                # 기본 할당\n",
    "    leftover = int(TARGET_N - base.sum())           # 남은 몫\n",
    "    # 가장 큰 소수점부터 1개씩 추가\n",
    "    remainders = (raw - base).sort_values(ascending=False)\n",
    "\n",
    "    alloc = base.copy()\n",
    "    for lbl in remainders.index[:leftover]:\n",
    "        alloc[lbl] += 1\n",
    "\n",
    "    # 각 그룹에서 할당 수만큼 샘플 (그룹 실제 크기보다 큰 경우 cap)\n",
    "    parts = []\n",
    "    for lbl, n_take in alloc.items():\n",
    "        grp = df[df[LABEL_COL] == lbl]\n",
    "        n_final = min(len(grp), int(n_take))\n",
    "        if n_final > 0:\n",
    "            parts.append(grp.sample(n=n_final, random_state=RS))\n",
    "    df = pd.concat(parts, axis=0).sample(frac=1.0, random_state=RS).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "# [아람EDITED]\n",
    "\n",
    "meta_cols = [c for c in df.columns if c not in ['text','audio_split_list','urgencyLevel','sentiment']]\n",
    "\n",
    "urgency_map = {'상': 2.0, '중': 1.0, '하': 0.0}\n",
    "sentiment_map = {'불안/걱정':0, '당황/난처':1, '중립':2, '기타부정':3}\n",
    "\n",
    "valid_size = int(len(df) * 0.2)\n",
    "df_train = df.iloc[:-valid_size].reset_index(drop=True)\n",
    "df_valid = df.iloc[-valid_size:].reset_index(drop=True)\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "model, scores = train_model(df_train, df_valid, meta_cols,\n",
    "                            TRAIN_AUDIO_TAR, VALID_AUDIO_TAR,\n",
    "                            urgency_map, sentiment_map)\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 10. 테스트/추론\n",
    "# =========================================\n",
    "print(\"========================= 테스트 =========================\")\n",
    "VALID_CSV_PATH = \"./data/validation/valid.csv\"\n",
    "VALID_AUDIO_TAR = \"./data/validation/total/valid_audio.tar\"\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "df_test = pd.read_csv(VALID_CSV_PATH)\n",
    "\n",
    "# 테스트용 Dataset 생성\n",
    "test_dataset = TextAudioDataset(df_test, meta_cols, VALID_AUDIO_TAR, urgency_map, sentiment_map)\n",
    "\n",
    "# DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 결과 저장 리스트 초기화\n",
    "preds_urgency, preds_sentiment = [], []\n",
    "labels_urgency, labels_sentiment = [], []\n",
    "\n",
    "# 테스트 루프\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Test\"):\n",
    "        # GPU 이동\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        audio_feat = batch['audio_feat'].to(device)\n",
    "        meta_idx = batch['meta_idx'].to(device)\n",
    "        label_urgency = batch['urgency'].to(device)\n",
    "        label_sentiment = batch['sentiment'].to(device)\n",
    "\n",
    "        # 예측\n",
    "        pred_urg, pred_sent = model(input_ids, attention_mask, audio_feat, meta_idx)\n",
    "\n",
    "        # 결과 저장\n",
    "        preds_urgency.extend(pred_urg.cpu().tolist())\n",
    "        preds_sentiment.extend(torch.argmax(pred_sent, dim=1).cpu().tolist())\n",
    "        labels_urgency.extend(label_urgency.cpu().tolist())\n",
    "        labels_sentiment.extend(label_sentiment.cpu().tolist())\n",
    "\n",
    "# 평가 지표 계산\n",
    "mae = mean_absolute_error(labels_urgency, preds_urgency)\n",
    "mse = mean_squared_error(labels_urgency, preds_urgency)\n",
    "acc = accuracy_score(labels_sentiment, preds_sentiment)\n",
    "f1 = f1_score(labels_sentiment, preds_sentiment, average='weighted')\n",
    "\n",
    "print(f\"Test MAE (urgencyLevel): {mae:.4f}\")\n",
    "print(f\"Test MSE (urgencyLevel): {mse:.4f}\")\n",
    "print(f\"Test Accuracy (sentiment): {acc:.4f}\")\n",
    "print(f\"Test Weighted F1 (sentiment): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
